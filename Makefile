# ğŸ¤– OpenAI API Simulator with SmolLM - Makefile
# This project simulates OpenAI API responses with optional real SmolLM PyTorch inference

BINARY=server
PORT?=8090
INFERENCE_PORT?=8081
OPENWEBUI_PORT?=3000
IMAGE?=openai-api-simulator:latest
IMAGE_BAKED?=openai-api-simulator:baked
SHELL := /bin/bash

# Default target - show help
.DEFAULT_GOAL := help

.PHONY: help setup-dev build run run-sim local-dev test tidy fmt clean \
        docker-build docker-run docker-build-baked docker-run-baked docker-clean \
        compose-up compose-down compose-up-noai compose-down-noai compose-logs compose-openwebui \
        curl-stream curl-text curl-sim open stop-bg run-sim-bg wait-for-api wait-for-ui

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“š HELP TARGET (DEFAULT)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

help:
	@echo ""
	@echo "  ğŸ¤– OpenAI API Simulator with SmolLM PyTorch Inference"
	@echo "  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "  ğŸ“¦ SETUP"
	@echo "     setup-dev                 â€¢ Initialize Go, Python, download SmolLM model"
	@echo ""
	@echo "  ğŸ”¨ LOCAL DEVELOPMENT"
	@echo "     build                     â€¢ Build Go API server (./server)"
	@echo "     run-sim                   â€¢ Run pure simulation (blocking)"
	@echo "     run-sim-bg                â€¢ Run pure simulation in background"
	@echo "     stop-bg                   â€¢ Stop background server"
	@echo "     local-dev                 â€¢ Run with real PyTorch inference (blocking)"
	@echo "     run                       â€¢ Run with default settings"
	@echo ""
	@echo "  ğŸ³ DOCKER (Single Service)"
	@echo "     docker-build              â€¢ Build image with PyTorch runtime"
	@echo "     docker-run                â€¢ Run container (API: 8090, Inference: 8081)"
	@echo "     docker-build-baked        â€¢ Build with SmolLM model embedded (~400MB)"
	@echo "     docker-run-baked          â€¢ Run baked image (faster startup)"
	@echo "     docker-clean              â€¢ Remove Docker images and containers"
	@echo ""
	@echo "  ğŸ³ DOCKER-COMPOSE (Complete Stack)"
	@echo "     compose-up                â€¢ Start: API + SmolLM + Web UI"
	@echo "     compose-down              â€¢ Stop the stack"
	@echo "     compose-up-noai           â€¢ Start: API + Web UI (no inference)"
	@echo "     compose-down-noai         â€¢ Stop the no-AI stack"
	@echo "     compose-logs              â€¢ Tail logs from all services"
	@echo "     compose-openwebui         â€¢ Start only the Web UI"
	@echo ""
	@echo "  ğŸ§ª TESTING & UTILITIES"
	@echo "     test                      â€¢ Run Go test suite"
	@echo "     curl-sim                  â€¢ Test pure simulation endpoint"
	@echo "     curl-stream               â€¢ Test streaming with SmolLM"
	@echo "     curl-text                 â€¢ Test non-streaming with SmolLM"
	@echo "     open                      â€¢ Open Web UI in browser (auto-waits for ready)"
	@echo "     fmt                       â€¢ Format Go code"
	@echo "     tidy                      â€¢ Tidy Go modules"
	@echo "     clean                     â€¢ Remove binaries"
	@echo ""
	@echo "  ğŸ”„ HEALTHCHECKS & WAITING"
	@echo "     wait-for-api              â€¢ Wait for API to be ready (max 60s)"
	@echo "     wait-for-ui               â€¢ Wait for Web UI to be ready (max 60s)"
	@echo ""
	@echo "  âš¡ QUICK START"
	@echo "     Fastest (Fake AI, 2 terminals):"
	@echo "        Terminal 1: make run-sim"
	@echo "        Terminal 2: make curl-sim"
	@echo ""
	@echo "     Fastest (Fake AI, single terminal):"
	@echo "        make run-sim-bg && make curl-sim && make stop-bg"
	@echo ""
	@echo "     Web UI + Fake AI (auto-waits for init):"
	@echo "        make compose-up-noai && make open"
	@echo ""
	@echo "     Web UI + Real AI (auto-waits for init):"
	@echo "        make compose-up && make open"
	@echo ""
	@echo "  ğŸ”§ ENVIRONMENT VARIABLES"
	@echo "     PORT=<n>                  â€¢ API port (default: 8090)"
	@echo "     INFERENCE_PORT=<n>        â€¢ Inference port (default: 8081)"
	@echo "     OPENWEBUI_PORT=<n>        â€¢ Web UI port (default: 3000)"
	@echo ""
	@echo "  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“¦ SETUP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

setup-dev:
	@echo "ğŸ”§ Setting up development environment..."
	@echo "   - Enabling GO111MODULE=on"
	go env -w GO111MODULE=on
	@echo "   - Downloading Go dependencies"
	go mod download
	@echo "   - Setting up Python environment"
	./scripts/setup-smollm.sh
	@echo "âœ… Development setup complete!"
	@echo ""
	@echo "Next steps:"
	@echo "  Terminal 1: python cmd/nanochat/inference_server.py --port 8081"
	@echo "  Terminal 2: go run ./cmd/server -port 8090"
	@echo "  Terminal 3: curl http://localhost:8090/v1/chat/completions ..."

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”¨ BUILD & RUN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

build:
	@echo "ğŸ”¨ Building Go API server..."
	go build -o $(BINARY) ./cmd/server
	@echo "âœ… Built: ./$(BINARY)"

run: build
	@echo "ğŸš€ Running API server on port $(PORT)..."
	./$(BINARY) -port $(PORT)

run-sim: build
	@echo "ğŸ§  Running pure simulation (fake AI, instant responses)"
	./$(BINARY) -port $(PORT)

run-sim-bg: build
	@echo "ğŸ§  Starting server in background on port $(PORT)..."
	./$(BINARY) -port $(PORT) > /tmp/openai-simulator.log 2>&1 &
	@echo "âœ… Server PID: $$!"
	@sleep 1
	@echo "   To view logs: tail -f /tmp/openai-simulator.log"
	@echo "   To stop: pkill -f '\\./(BINARY)' or use: make stop-bg"

local-dev: build
	@echo "ğŸš€ Starting local development..."
	@echo "   API port: $(PORT)"
	@echo "   Inference port: $(INFERENCE_PORT)"
	@echo ""
	@echo "âš ï¸  Make sure Python inference server is running:"
	@echo "   python cmd/nanochat/inference_server.py --port $(INFERENCE_PORT)"
	@echo ""
	./$(BINARY) -port $(PORT)

stop-bg:
	@echo "ğŸ›‘ Stopping background server..."
	@pkill -f '\./$(BINARY)' || echo "âœ… No server running"
	@echo "âœ… Stopped"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§ª TESTING & CODE QUALITY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

test:
	@echo "ğŸ§ª Running tests..."
	go test ./... -v

fmt:
	@echo "ğŸ“ Formatting Go code..."
	gofmt -w .

tidy:
	@echo "ğŸ“¦ Tidying Go modules..."
	go mod tidy

clean:
	@echo "ğŸ§¹ Cleaning up..."
	rm -f $(BINARY)
	@echo "âœ… Cleaned"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ³ DOCKER (Single Service)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

docker-build:
	@echo "ğŸ“¦ Building Docker image: $(IMAGE)"
	DOCKER_BUILDKIT=1 docker build -t $(IMAGE) .
	@echo "âœ… Built: $(IMAGE)"

docker-run: docker-build
	@echo "ğŸ³ Running Docker image"
	@echo "   API: http://localhost:$(PORT)"
	@echo "   Inference: http://localhost:$(INFERENCE_PORT)"
	docker run --rm -p $(PORT):8090 -p $(INFERENCE_PORT):8081 \
		--name openai-api-simulator \
		-e PYTHONUNBUFFERED=1 \
		$(IMAGE)

docker-build-baked:
	@echo "ğŸ“¦ Building Docker image with baked SmolLM model..."
	@echo "âš ï¸  This may take 5-10 minutes (downloads 386MB GGUF)"
	DOCKER_BUILDKIT=1 docker build --build-arg BAKED=true -t $(IMAGE_BAKED) .
	@echo "âœ… Built: $(IMAGE_BAKED)"

docker-run-baked: docker-build-baked
	@echo "ğŸ³ Running baked Docker image (faster startup)"
	@echo "   API: http://localhost:$(PORT)"
	@echo "   Inference: http://localhost:$(INFERENCE_PORT)"
	docker run --rm -p $(PORT):8090 -p $(INFERENCE_PORT):8081 \
		--name openai-api-simulator \
		-e PYTHONUNBUFFERED=1 \
		$(IMAGE_BAKED)

docker-clean:
	@echo "ğŸ§¹ Cleaning Docker images and containers..."
	-docker stop openai-api-simulator || true
	-docker rmi $(IMAGE) || true
	-docker rmi $(IMAGE_BAKED) || true
	@echo "âœ… Cleanup complete"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ³ DOCKER-COMPOSE (Complete Stack)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

wait-for-api:
	@echo "â³ Waiting for API to be ready on port $(PORT)..."
	@for i in {1..60}; do \
		if curl -s http://localhost:$(PORT)/health > /dev/null 2>&1; then \
			echo "âœ… API is ready"; \
			exit 0; \
		fi; \
		echo -n "."; \
		sleep 1; \
	done; \
	echo ""; \
	echo "âŒ API failed to start"; \
	exit 1

wait-for-ui:
	@echo "â³ Waiting for Web UI to be ready on port $(OPENWEBUI_PORT)..."
	@for i in {1..60}; do \
		if curl -s http://localhost:$(OPENWEBUI_PORT) > /dev/null 2>&1; then \
			echo "âœ… Web UI is ready"; \
			exit 0; \
		fi; \
		echo -n "."; \
		sleep 1; \
	done; \
	echo ""; \
	echo "âŒ Web UI failed to start"; \
	exit 1

compose-up:
	@echo "ğŸ³ Starting Docker Compose stack..."
	@echo "   Services: API + SmolLM Inference + Open Web UI"
	docker compose up --build -d
	@echo ""
	@echo "âœ… Stack started! Waiting for services to initialize..."
	@$(MAKE) wait-for-api
	@$(MAKE) wait-for-ui
	@echo ""
	@echo "ğŸ‰ All services ready!"
	@echo "   API: http://localhost:$(PORT)"
	@echo "   Inference: http://localhost:$(INFERENCE_PORT)"
	@echo "   Web UI: http://localhost:$(OPENWEBUI_PORT)"
	@echo ""
	@echo "ğŸ’¡ Tip: make open"

compose-down:
	@echo "ğŸ›‘ Stopping Docker Compose stack..."
	docker compose down
	@echo "âœ… Stopped"

compose-up-noai:
	@echo "ğŸ³ Starting Docker Compose (without SmolLM)..."
	@echo "   Services: API + Open Web UI (pure simulation)"
	docker compose -f docker-compose.noai.yml up --build -d
	@echo ""
	@echo "âœ… Stack started! Waiting for services to initialize..."
	@$(MAKE) wait-for-api
	@$(MAKE) wait-for-ui
	@echo ""
	@echo "ğŸ‰ All services ready!"
	@echo "   API: http://localhost:$(PORT)"
	@echo "   Web UI: http://localhost:$(OPENWEBUI_PORT)"
	@echo ""
	@echo "ğŸ’¡ Tip: make open"

compose-down-noai:
	@echo "ğŸ›‘ Stopping Docker Compose (no-AI)..."
	docker compose -f docker-compose.noai.yml down
	@echo "âœ… Stopped"

compose-logs:
	@echo "ğŸ“‹ Tailing Docker Compose logs (last 100 lines)..."
	docker compose logs -f --tail=100

compose-openwebui:
	@echo "ğŸ³ Starting Open Web UI service..."
	docker compose up -d openwebui
	@echo "âœ… Started on http://localhost:$(OPENWEBUI_PORT)"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”— UTILITIES & API TESTING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

open:
	@echo "ğŸŒ Opening Web UI in browser..."
	@$(MAKE) wait-for-ui
	@open http://localhost:$(OPENWEBUI_PORT) || xdg-open http://localhost:$(OPENWEBUI_PORT) || echo "Please open http://localhost:$(OPENWEBUI_PORT)"

curl-sim:
	@echo "ğŸ§  Testing pure simulation endpoint..."
	@echo ""
	curl -s -X POST http://localhost:$(PORT)/v1/chat/completions \
		-H "Content-Type: application/json" \
		-d '{"model":"gpt-4","messages":[{"role":"user","content":"Generate a fun fact about AI."}],"stream":true}' | head -20

curl-stream:
	@echo "ğŸ“¡ Testing streaming with SmolLM..."
	@echo ""
	curl -s -X POST http://localhost:$(PORT)/v1/chat/completions \
		-H "Content-Type: application/json" \
		-d '{"model":"smollm","messages":[{"role":"user","content":"Say hello in 10 words or less."}],"stream":true}' | head -20

curl-text:
	@echo "ğŸ“ Testing non-streaming with SmolLM..."
	@echo ""
	curl -s -X POST http://localhost:$(PORT)/v1/chat/completions \
		-H "Content-Type: application/json" \
		-d '{"model":"smollm","messages":[{"role":"user","content":"Explain AI in one sentence."}],"stream":false}' | head -20
