#!/usr/bin/env python3
"""
SmolLM PyTorch Inference Server

Provides OpenAI-compatible chat completion API using SmolLM models.
This server bridges the Go simulator to SmolLM model inference.

Usage:
    python cmd/nanochat/inference_server.py --port 8081 --model smollm

Features:
    - Streaming Server-Sent Events for token generation
    - Automatic device selection (CUDA, MPS, CPU)
    - Efficient KV cache management
    - OpenAI-compatible /chat/completions endpoint
"""

import argparse
import asyncio
import json
import logging
import sys
import torch
from typing import AsyncGenerator, Optional, List

try:
    from fastapi import FastAPI, HTTPException
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.responses import StreamingResponse
    from pydantic import BaseModel
    import uvicorn
    from transformers import AutoTokenizer, AutoModelForCausalLM
except ImportError:
    print("Error: Required packages not installed.")
    print("Install with: pip install fastapi uvicorn torch transformers pydantic")
    sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# Data Models
# ============================================================================

class ChatMessage(BaseModel):
    """OpenAI-compatible chat message format."""
    role: str  # "user" or "assistant"
    content: str


class ChatCompletionRequest(BaseModel):
    """OpenAI-compatible chat completion request."""
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512
    top_k: Optional[int] = None
    model: Optional[str] = "smollm"


# ============================================================================
# Device & Model Management
# ============================================================================

def autodetect_device() -> tuple[str, torch.device]:
    """Auto-detect best available device."""
    if torch.cuda.is_available():
        device_type = "cuda"
        device = torch.device("cuda:0")
        logger.info(f"Using CUDA device: {torch.cuda.get_device_name(0)}")
    elif torch.backends.mps.is_available():
        device_type = "mps"
        device = torch.device("mps")
        logger.info("Using Metal Performance Shaders (MPS) on Apple Silicon")
    else:
        device_type = "cpu"
        device = torch.device("cpu")
        logger.info("Using CPU for inference")
    
    return device_type, device


def load_smollm_model(model_name: str, device: torch.device):
    """
    Load SmolLM model and tokenizer from HuggingFace.
    
    Args:
        model_name: HuggingFace model identifier (e.g., "HuggingFaceTB/SmolLM-135M")
        device: torch.device to load model onto
    
    Returns:
        (model, tokenizer) tuple
    """
    logger.info(f"Loading SmolLM model: {model_name}")
    
    try:
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        logger.info(f"Tokenizer loaded from {model_name}")
        
        # Load model with auto device mapping when available
        if device.type == "cuda":
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,  # Use FP16 for faster inference on CUDA
                device_map="auto"
            )
        else:
            # Load to CPU or MPS without device_map
            model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
        
        model.eval()  # Set to evaluation mode
        logger.info(f"Model loaded successfully on {device}")
        
        return model, tokenizer
    
    except Exception as e:
        logger.error(f"Failed to load model {model_name}: {e}")
        raise


class SmolLMInference:
    """Wrapper for SmolLM inference with PyTorch."""
    
    def __init__(self, model, tokenizer, device: torch.device, device_type: str):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.device_type = device_type
    
    async def stream_completion(
        self,
        messages: List[ChatMessage],
        temperature: float = 0.7,
        max_tokens: int = 512,
        top_k: Optional[int] = None
    ) -> AsyncGenerator[str, None]:
        """
        Stream chat completion tokens as JSON lines.
        
        Yields Server-Sent Events format: "data: {json}\n\n"
        """
        try:
            # Build conversation text from messages
            conversation_text = self._build_conversation_text(messages)
            
            logger.info(f"Generating completion with temperature={temperature}, max_tokens={max_tokens}")
            
            # Tokenize input
            inputs = self.tokenizer(conversation_text, return_tensors="pt").to(self.device)
            input_length = inputs["input_ids"].shape[1]
            
            # Generate tokens
            with torch.no_grad():
                # Use generate with streaming
                output_ids = self.model.generate(
                    inputs["input_ids"],
                    attention_mask=inputs.get("attention_mask"),
                    max_length=input_length + max_tokens,
                    temperature=temperature,
                    top_k=top_k if top_k else 50,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                )
            
            # Decode and stream tokens one by one
            # Get only the generated part (skip input tokens)
            generated_ids = output_ids[0, input_length:]
            
            for token_id in generated_ids:
                # Decode single token
                token_text = self.tokenizer.decode([token_id], skip_special_tokens=True)
                
                if token_text.strip():  # Only yield non-empty tokens
                    yield f"data: {json.dumps({'token': token_text})}\n\n"
                    logger.debug(f"Token: {repr(token_text)}")
                
                # Yield control occasionally
                await asyncio.sleep(0)
            
            yield "data: {\"done\": true}\n\n"
            logger.info(f"Completion finished: {len(generated_ids)} tokens generated")
        
        except Exception as e:
            logger.error(f"Error during generation: {e}", exc_info=True)
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    def _build_conversation_text(self, messages: List[ChatMessage]) -> str:
        """Convert chat messages to conversation text."""
        # SmolLM doesn't have special tokens like NanoChat, so we use simple formatting
        conversation_parts = []
        
        for msg in messages:
            if msg.role == "user":
                conversation_parts.append(f"User: {msg.content}")
            elif msg.role == "assistant":
                conversation_parts.append(f"Assistant: {msg.content}")
        
        # Add assistant prompt for generation
        conversation_parts.append("Assistant:")
        
        return "\n".join(conversation_parts)


# ============================================================================
# FastAPI Application
# ============================================================================

def create_app(model_name: str = "HuggingFaceTB/SmolLM-135M", device_type: Optional[str] = None) -> FastAPI:
    """Create and configure FastAPI application."""
    
    app = FastAPI(title="SmolLM Inference Server", version="0.1.0")
    
    # Add CORS middleware for external clients
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Device selection
    if device_type:
        if device_type == "cuda":
            selected_device = torch.device("cuda:0")
            selected_device_type = "cuda"
        elif device_type == "mps":
            selected_device = torch.device("mps")
            selected_device_type = "mps"
        elif device_type == "cpu":
            selected_device = torch.device("cpu")
            selected_device_type = "cpu"
        else:
            raise ValueError(f"Unsupported device type: {device_type}")
    else:
        selected_device_type, selected_device = autodetect_device()
    
    # Model and inference engine
    inference_engine: Optional[SmolLMInference] = None
    model_error: Optional[str] = None
    
    # Startup event
    @app.on_event("startup")
    async def startup():
        nonlocal inference_engine, model_error
        try:
            logger.info(f"Loading SmolLM model: {model_name}")
            model, tokenizer = load_smollm_model(model_name, selected_device)
            
            inference_engine = SmolLMInference(
                model,
                tokenizer,
                selected_device,
                selected_device_type
            )
            logger.info("Model loaded successfully")
        except Exception as e:
            model_error = str(e)
            logger.error(f"Failed to load model: {e}")
    
    # Health check endpoint
    @app.get("/health")
    async def health():
        if model_error:
            return {
                "status": "error",
                "message": f"Model load failed: {model_error}",
                "ready": False
            }
        return {
            "status": "ok",
            "ready": inference_engine is not None,
            "device": str(selected_device),
            "device_type": selected_device_type
        }
    
    # Chat completion endpoint
    @app.post("/chat/completions")
    async def chat_completions(request: ChatCompletionRequest):
        """OpenAI-compatible chat completions endpoint (streaming)."""
        
        if model_error:
            raise HTTPException(status_code=503, detail=f"Model not ready: {model_error}")
        
        if not inference_engine:
            raise HTTPException(status_code=503, detail="Inference engine not initialized")
        
        # Validate inputs
        if not request.messages:
            raise HTTPException(status_code=400, detail="No messages provided")
        
        if len(request.messages) > 500:
            raise HTTPException(status_code=400, detail="Too many messages (max 500)")
        
        # Clamp parameters
        temperature = max(0.0, min(2.0, request.temperature or 0.7))
        max_tokens = max(1, min(4096, request.max_tokens or 512))
        top_k = request.top_k if request.top_k else None
        
        # Log request
        logger.info(f"Chat request: {len(request.messages)} messages")
        for i, msg in enumerate(request.messages[-3:]):  # Log last 3 messages
            logger.info(f"  [{msg.role}] (message {i}): {msg.content[:100]}")
        
        # Create streaming response
        return StreamingResponse(
            inference_engine.stream_completion(
                request.messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_k=top_k
            ),
            media_type="text/event-stream"
        )
    
    # Info endpoint
    @app.get("/info")
    async def info():
        return {
            "name": "smollm-inference",
            "version": "0.1.0",
            "model": model_name,
            "device": str(selected_device),
            "device_type": selected_device_type,
            "torch_version": torch.__version__
        }
    
    return app


# ============================================================================
# Main Entry Point
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="SmolLM Inference Server"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8081,
        help="Port to run server on (default: 8081)"
    )
    parser.add_argument(
        "--host",
        type=str,
        default="0.0.0.0",
        help="Host to bind to (default: 0.0.0.0)"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="HuggingFaceTB/SmolLM-135M",
        help="HuggingFace model identifier (default: HuggingFaceTB/SmolLM-135M)"
    )
    parser.add_argument(
        "--device",
        type=str,
        choices=["cuda", "mps", "cpu"],
        default=None,
        help="Force specific device (cuda, mps, cpu). Auto-detect if not specified."
    )
    
    args = parser.parse_args()
    
    # Create app with specified model
    app = create_app(model_name=args.model, device_type=args.device)
    
    # Start server
    logger.info(f"Starting SmolLM inference server on {args.host}:{args.port}")
    logger.info(f"Model: {args.model}")
    
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level="info"
    )


if __name__ == "__main__":
    main()
