services:
  simulator:
    build: .
    image: openai-api-simulator:latest
    ports:
      - "3080:3080"
    environment:
      - GIN_MODE=release
      # Default streaming behavior for a realistic local dev environment
      # jitter: randomized per-chunk delay between STREAM_DELAY_MIN_MS
      # and STREAM_DELAY_MAX_MS (in ms).
      - STREAM_DELAY_MIN_MS=50
      - STREAM_DELAY_MAX_MS=300
      # Approximate token emission rate for streaming chunks. ~40 tokens/sec
      # mirrors rough throughput for modern LLMs in local testing.
      - STREAM_TOKENS_PER_SECOND=40
      # Default response length option used when client doesn't set
      # `response_length`. Supported: short|medium|long. Empty (default)
      # lets the simulator infer length from the prompt.
      - STREAM_DEFAULT_RESPONSE_LENGTH=medium

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    ports:
      - "3000:8080" # open-webui default UI port
    environment:
      # Tell Open WebUI to use the simulator as an OpenAI-compatible backend
      - OPENAI_API_BASE_URL=http://simulator:3080
      # Open WebUI expects a key to be set when using OpenAI API flow; the simulator does not validate keys
      - OPENAI_API_KEY=simulator
      # Disable auth for quick local development
      - WEBUI_AUTH=False
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      - simulator

volumes:
  open-webui:
